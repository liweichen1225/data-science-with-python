{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Wed May 13 @ 11:59pm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework we will covering NLP, Topic Modeling and Recommendation Engines\n",
    "\n",
    "We will generate recommendations on products from a department store based on product descriptions.\n",
    "We'll first transform the data into topics using Latent Dirichlet Approximation, and then generate recommendations based on this new representation.\n",
    "\n",
    "\n",
    "Instructions\n",
    "Follow the comments below and fill in the blanks (____) to complete.\n",
    "\n",
    "**Please 'Restart and Run All' prior to submission.**\n",
    "\n",
    "**When submitting to Gradescope, please mark on which page each question is answered.**\n",
    "\n",
    "Out of 26 points total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA and Recommendation Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a recommendation engine for products from a department store.  \n",
    "The recommendations will be based on the similarity of product descriptions.  \n",
    "We'll query a product and get back a list of products that are similar.  \n",
    "Instead of using the descriptions directly, we will first do some topic modeling using LDA to transform the descriptions into a topic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.info of                                uniq_id           sku  \\\n",
       "0     b6c0b6bea69c722939585baeac73c13d  pp5006380337   \n",
       "1     8ffd0ef4fcaf1a82fb514aba5d20e05b  pp5006790247   \n",
       "2     4d9337e3c8f974d3c420cdc5c58b3fc3  pp5007090172   \n",
       "3     5abf9d28e9e0404369ece10807d99d0e  pp5006790242   \n",
       "4     3c291110238ee460390c7002e4622ade  pp5006790695   \n",
       "...                                ...           ...   \n",
       "4995  d6c9485a403998f60cf76d03b7e43f73  pp5005901018   \n",
       "4996  c78a231cbb474c49be916e88fd209fea  pp5006480763   \n",
       "4997  5ff46580a73ef5c301e6ef33f6092759       1562c2b   \n",
       "4998  707fc4fe51bb5dd3b8b05b906f15ec3a  pp5005860259   \n",
       "4999  6ea61a3c3dd3ae7257f26fa05dbcaf40  pp5006461700   \n",
       "\n",
       "                                             name_title  \\\n",
       "0           Alfred Dunner® Essential Pull On Capri Pant   \n",
       "1     Alfred Dunner® Feels Like Spring 3/4 Sleeve Le...   \n",
       "2     Alfred Dunner® Feels Like Spring 3/4-Sleeve Le...   \n",
       "3               Alfred Dunner® Feels Like Spring Capris   \n",
       "4     Alfred Dunner® Feels Like Spring Short-Sleeve ...   \n",
       "...                                                 ...   \n",
       "4995              Carter's® Elephant Blanket - One Size   \n",
       "4996  Maxwell & Williams™ Sprinkle Polka Dot 6-pc. S...   \n",
       "4997    Wolverine® Spencer Mens Waterproof Hiking Boots   \n",
       "4998   Lugz® Lumber Hi Mens Slip-Resistant Hiking Boots   \n",
       "4999  Okie Dokie® Graphic Trapeze Tank Top - Prescho...   \n",
       "\n",
       "                                            description        category  \\\n",
       "0     You'll return to our Alfred Dunner pull-on cap...   alfred dunner   \n",
       "1     For easygoing style you'll love, wear our stre...        view all   \n",
       "2     Spring is in the air with our 3/4-sleeve leaf ...        skechers   \n",
       "3     Pull on a pair of our casual capris and be foo...  capris & crops   \n",
       "4     Timeless and stylish, our short-sleeve burnout...            tops   \n",
       "...                                                 ...             ...   \n",
       "4995  Your girl will love snuggling up in this ultra...             NaN   \n",
       "4996  With its pleasant vintage look and texture, th...             NaN   \n",
       "4997  Featuring aggressive tread, sturdy reinforced ...             NaN   \n",
       "4998  Say goodbye to slipping on the trail with thes...             NaN   \n",
       "4999  This graphic trapeze tank top is full of perso...             NaN   \n",
       "\n",
       "                      category_tree  \n",
       "0      jcpenney|women|alfred dunner  \n",
       "1           jcpenney|women|view all  \n",
       "2           jcpenney|women|skechers  \n",
       "3     jcpenney|women|capris & crops  \n",
       "4               jcpenney|women|tops  \n",
       "...                             ...  \n",
       "4995                            NaN  \n",
       "4996                            NaN  \n",
       "4997                            NaN  \n",
       "4998                            NaN  \n",
       "4999                            NaN  \n",
       "\n",
       "[5000 rows x 6 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions from JCPenney.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# This is compressed version of a csv file.\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# read_csv has a parameter compression with default value 'infer' that will handle unzipping the data.\n",
    "# Store the resulting dataframe as df_products.\n",
    "df_products=pd.read_csv('../data/jcpenney-products_subset.csv.zip')\n",
    "\n",
    "# print a summary of df_products using .info, noting the number of records (should be 5000)\n",
    "df_products.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfred Dunner® Essential Pull On Capri Pant\n",
      "You'll return to our Alfred Dunner pull-on capris again and again when you want an updated, casual look and all the comfort you love.   elastic waistband approx. 19-21\" inseam slash pockets polyester washable imported      \n"
     ]
    }
   ],
   "source": [
    "# 2. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   name_title which is the name of the product stored as a string\n",
    "#   description which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the product name_title in row 0 of df_products\n",
    "print(df_products.name_title[0])\n",
    "\n",
    "# print the product desciption in row 0 of df_products\n",
    "print(df_products.description[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3979)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first need to vectorize from strings to \n",
    "#   fixed vectors of floats.\n",
    "# To do this we will transform our documents into unigrams using Tf-Idf,\n",
    "#    use both unigrams and bigrams\n",
    "#    excluding terms which appear in less than 10 documents\n",
    "#    excluding common English stop words and\n",
    "\n",
    "# Import TfidfVectorizer from sklearn.feature_extraction.text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#  Instantiate a TfidfVectorizer with\n",
    "#   ngram_range=(1,2),\n",
    "#   min_df=10,\n",
    "#   stop_words='english'\n",
    "# Store as tfidf\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,2),min_df=10,stop_words='english')\n",
    "\n",
    "# fit_transform tfidf on the descriptions column of our dataframe, creating the transformed dataset X_tfidf\n",
    "# Store as X_tfidf\n",
    "X_tfidf=tfidf.fit_transform(df_products.description)\n",
    "\n",
    "# Print the shape of X_tfidf (should be 5000 x 3979)\n",
    "X_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['zipper_pockets', 'zippered', 'zippers', 'zirconia', 'zone']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. (3pts) Format Bigram Labels and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The extracted vocabulary cat be retrieved from tfidf as a list using get_feature_names()\n",
    "# Store the extracted vocabulary as vocabulary\n",
    "vocabulary=tfidf.get_feature_names()\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make output easier to read, replace all spaces in our vocabulary list with underscores.\n",
    "# To do this we can use the string replace() method.\n",
    "# For example x.replace(' ','_') with replace all ' ' in x with '_'.\n",
    "# Store the result back in vocabulary.\n",
    "vocabulary=[x.replace(' ','_') for x in vocabulary]\n",
    "\n",
    "# Print the last 5 terms in the vocabulary\n",
    "vocabulary[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. (4pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   per-document topic distributions and per-topic term distributions.\n",
    "# Though there are likely more, we'll model our dataset using 20 topics to keep things small.\n",
    "# We'd like the model to run on all of the cores available in the machine we're using.\n",
    "#    `n_jobs` tells the model how many cores to use, while `n_jobs=-1` indicates use all available.\n",
    "# We'd also like the results to always be the same, so set random_state=123\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn.decomposition\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model with\n",
    "#    n_components=20, n_jobs=-1, random_state=123\n",
    "# Store as lda\n",
    "lda=LatentDirichletAllocation(n_components=20, n_jobs=-1, random_state=123)\n",
    "\n",
    "# Run fit_transform on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "# NOTE: this step may take a minute or more depending on your setup.\n",
    "X_lda=lda.fit_transform(X_tfidf)\n",
    "\n",
    "# Print the shape of the X_lda (should be 5000 x 20)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0 upper, sole, rubber, synthetic, rubber_sole\n",
      "Topic #1 fringe, cups, swim, band, partially_lined\n",
      "Topic #2 wash, dry, line, line_dry, wash_line\n",
      "Topic #3 length, approx, washable_imported, washable, length_polyester\n",
      "Topic #4 metal, jewelry, photos, enlarged, photos_enlarged\n",
      "Topic #5 flats, upper, sole, thermoplastic, upper_lace\n",
      "Topic #6 elastane, seat, thigh, elastane_washable, belt\n",
      "Topic #7 clean, imported, measures, easy, wipe\n",
      "Topic #8 wicking, moisture, moisture_wicking, safe, dri\n",
      "Topic #9 wedge, button_placket, upper, resistant, textile\n",
      "Topic #10 support, bra, imported_manufacturer, manufacturer_style, manufacturer\n",
      "Topic #11 clean_imported, clean, spot, spot_clean, polyester\n",
      "Topic #12 cotton, sleeves, washable_imported, washable, short\n",
      "Topic #13 cool_casual, tee_features, adhesive, self_adhesive, faux_drawstring\n",
      "Topic #14 pockets, button, fly, cotton, closure\n",
      "Topic #15 rug, resistant, yes, indoor, backing\n",
      "Topic #16 wine, sunglasses, dry_america, frames, drawers\n",
      "Topic #17 stainless, stainless_steel, steel, quart, cargo_shorts\n",
      "Topic #18 inseam, spandex, spandex_washable, elastic, washable_imported\n",
      "Topic #19 dial, case, strap, watch, width\n"
     ]
    }
   ],
   "source": [
    "# 6. (4pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# We'd like a print statement that looks like this:\n",
    "#    Topic #0 upper sole rubber synthetic rubber_sole\n",
    "#\n",
    "# For each topic print 'Topic #{idx} ' followed by the top 5 most likely terms in that topic.\n",
    "# Hints: \n",
    "#   Use vocabulary created above, but first convert from a list to np.array to make indexing easier\n",
    "#   The per topic term distributions are stored in model.components_\n",
    "#   np.argsort returns the indices of an np.array sorted by their value, in ascending order\n",
    "#   [::-1] reverses the order of an np.array\n",
    "topic_words = {}\n",
    "terms=np.array(vocabulary)\n",
    "for topic, comp in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:5]\n",
    "    topic_words[topic] = [terms[i] for i in word_idx]\n",
    "for topic, words in topic_words.items():\n",
    "    print(f'Topic #{topic} '+', '.join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above.\n",
    "# We'd like to recommend similar products in LDA space.\n",
    "# We'll use cosine_similarity as measure of similarity.\n",
    "\n",
    "# From sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Use cosine_similarity to generate similarity scores on our X_lda data\n",
    "# Store as similarities.\n",
    "# NOTE: we only need to pass X_lda in once,\n",
    "#   the function will calculate pairwise similarity for all elements in that matrix\n",
    "similarities=cosine_similarity(X_lda)\n",
    "\n",
    "# print the shape of the similarities matrix (should be 5000x5000)\n",
    "similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             Alfred Dunner® Essential Pull On Capri Pant\n",
       "2091                  Alfred Dunner® Pull-On Pants - Plus\n",
       "3620                         Alfred Dunner® Pull-On Pants\n",
       "662                           Alfred Dunner® Pull On Pant\n",
       "2246      Alfred Dunner® Sao Paolo Pull-On Pants - Petite\n",
       "3390                        Stylus™ Stretch Bootcut Jeans\n",
       "1146           St. John's Bay® Linen Cropped Pants - Plus\n",
       "1578    City Streets® Colorblock Performance Cropped L...\n",
       "1838                                 Stylus™ Linen Shorts\n",
       "3169                   Liz Claiborne® Jogger Pants - Tall\n",
       "Name: name_title, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.(4pts) Generate Recommendations\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_products.\n",
    "#   The name of this product is \"Alfred Dunner® Essential Pull On Capri Pant\"\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the cosine similarities from row 0 of the similarities matrix\n",
    "#   get the indices of this array sorted by value using np.argsort\n",
    "#   reverse the order of these indices (remember, we want high values and np.argsort evaluates ascending)\n",
    "#   get the first 10 indexes from this reversed array\n",
    "#   use those indices to index into df_products.name_title and print the result\n",
    "\n",
    "# HINT: The first two products should be:\n",
    "#   'Alfred Dunner® Essential Pull On Capri Pant', (the original query product)\n",
    "#   'Alfred Dunner® Pull-On Pants - Plus',\n",
    "df_products.name_title[np.argsort(similarities[0,:])[::-1][:10]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
